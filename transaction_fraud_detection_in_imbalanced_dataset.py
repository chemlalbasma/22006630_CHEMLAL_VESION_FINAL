# -*- coding: utf-8 -*-
"""transaction-fraud-detection-in-imbalanced-dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OKlEymvv7pOZ-v4WxUSv2wynGFR4AXDG

# Fraud Detection using different sampling methods
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("/kaggle/input/paysim1/PS_20174392719_1491204439457_log.csv")
df.head()

df.info()

df.describe()

df.isnull().sum()

df['type'].value_counts()

df[df['isFraud']==1]['type'].value_counts()

sns.countplot(x='type', data=df)
plt.show()

sns.displot(data=df, x="amount", kind="kde")

sns.displot(data=df[df['isFraud']==1], x="amount", kind="kde")

df['isFraud'].value_counts()

fraud_val = df['isFraud'].value_counts().values
plt.pie(fraud_val, labels = ['NotFraud','Fraud'],autopct='%.2f%%')

df[df['isFlaggedFraud']==1]

"""The above df indicates that all flagged txns are fraud"""

df[df['isFlaggedFraud']==1].amount.min()

df[df['isFlaggedFraud']==1]['type'].value_counts()

sns.boxplot(x='isFraud', y='amount', data=df)
plt.title("Transaction Amount vs. Fraud")
plt.show()

plt.figure(figsize=(8, 8))
col = df[['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig',
        'oldbalanceDest', 'newbalanceDest', 'isFraud',
       'isFlaggedFraud']]
corr = col.corr()
sns.heatmap(corr,annot=True, cmap='coolwarm')

"""Based on above analysis... found that nothing can be done with such imbalanced data.

Two options:
1 undersample the majority
2 oversample the minority
"""

df.columns

df.drop(['step', 'nameOrig', 'nameDest'], axis=1, inplace=True)

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler

trans_type = pd.get_dummies(df.type)
trans_type = trans_type.astype(int)
trans_type.head()

df = trans_type.join(df)
df.drop(['type'], axis = 1, inplace = True)

cols = ['isFraud']
cols.extend([col for col in df.columns if col != 'isFraud'])
df = df[cols]

df.head()

X = df.drop(['isFraud'], axis=1)
y = df['isFraud']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression, Perceptron
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, log_loss, roc_curve, auc
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)
print("DT Model: \n",classification_report(y_test, dt_pred))
print(confusion_matrix(y_test, dt_pred))

nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
nb_pred = nb_model.predict(X_test)
print("NB Model: \n",classification_report(y_test, nb_pred))
print(confusion_matrix(y_test, nb_pred))

lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
print("LR Model: \n",classification_report(y_test, lr_pred))
print(confusion_matrix(y_test, lr_pred))

perceptron_model = Perceptron(max_iter=1000)
perceptron_model.fit(X_train, y_train)
perceptron_pred = perceptron_model.predict(X_test)
print("Perceptron Model: \n",classification_report(y_test, perceptron_pred))
print(confusion_matrix(y_test, perceptron_pred))

df2 = df

X = df2.drop(['isFraud'], axis=1)
y = df2['isFraud']

smote = SMOTE(random_state=42)
X, y = smote.fit_resample(X, y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)
print("DT Model: \n",classification_report(y_test, dt_pred))
print(confusion_matrix(y_test, dt_pred))

nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
nb_pred = nb_model.predict(X_test)
print("NB Model: \n",classification_report(y_test, nb_pred))
print(confusion_matrix(y_test, nb_pred))

"""precision of class 1 increased..."""

lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
print("LR Model: \n",classification_report(y_test, lr_pred))
print(confusion_matrix(y_test, lr_pred))

perceptron_model = Perceptron(max_iter=1000)
perceptron_model.fit(X_train, y_train)
perceptron_pred = perceptron_model.predict(X_test)
print("Perceptron Model: \n",classification_report(y_test, perceptron_pred))
print(confusion_matrix(y_test, perceptron_pred))

plt.figure(figsize=(20,15))
fpr, tpr, _ = roc_curve(y_test, lr_pred)
roc_auc = auc(fpr, tpr)
plt.subplot(2, 2, 1)
plt.plot(fpr, tpr, lw=2, label=f'(area = {roc_auc:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Logistic Regression')
plt.legend(loc='lower right')

fpr, tpr, _ = roc_curve(y_test, nb_pred)
roc_auc = auc(fpr, tpr)
plt.subplot(2, 2, 2)
plt.plot(fpr, tpr, lw=2, label=f'(area = {roc_auc:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Naive Bayes')
plt.legend(loc='lower right')

fpr, tpr, _ = roc_curve(y_test, perceptron_pred)
roc_auc = auc(fpr, tpr)
plt.subplot(2, 2, 3)
plt.plot(fpr, tpr, lw=2, label=f'(area = {roc_auc:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Perceptron')
plt.legend(loc='lower right')

fpr, tpr, _ = roc_curve(y_test, dt_pred)
roc_auc = auc(fpr, tpr)
plt.subplot(2, 2, 4)
plt.plot(fpr, tpr, lw=2, label=f'area = {roc_auc:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Decision Tree')
plt.legend(loc='lower right')

from sklearn.model_selection import cross_val_score

# lr_model = LogisticRegression(random_state=3)
# nb_model = GaussianNB()
# perceptron_model = Perceptron(max_iter=1000, random_state=3)
# dt_model = DecisionTreeClassifier(random_state=3)

# k = 5

# lr_cv_scores = cross_val_score(lr_model, X_train, y_train, cv=k, scoring='accuracy')
# nb_cv_scores = cross_val_score(nb_model, X_train, y_train, cv=k, scoring='accuracy')
# perceptron_cv_scores = cross_val_score(perceptron_model, X_train, y_train, cv=k, scoring='accuracy')
# dt_cv_scores = cross_val_score(dt_model, X_train, y_train, cv=k, scoring='accuracy')

# print(f'Logistic Regression (cv): {lr_cv_scores.mean():.2f}')
# print(f'Naive Bayes (cv): {nb_cv_scores.mean():.2f}')
# print(f'Perceptron (cv): {perceptron_cv_scores.mean():.2f}')
# print(f'Decision Tree (cv): {dt_cv_scores.mean():.2f}')

"""Undersampling"""

df3 = df

class_0_samples = df3[df3['isFraud'] == 0].sample(n=8213, random_state=42)
class_1_samples = df3[df3['isFraud'] == 1].sample(n=8213, random_state=42)
df3 = pd.concat([class_0_samples, class_1_samples])

X = df3.drop(['isFraud'], axis=1)
y = df3['isFraud']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)
print("DT Model: \n",classification_report(y_test, dt_pred))
print(confusion_matrix(y_test, dt_pred))

nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
nb_pred = nb_model.predict(X_test)
print("NB Model: \n",classification_report(y_test, nb_pred))
print(confusion_matrix(y_test, nb_pred))

lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
print("LR Model: \n",classification_report(y_test, lr_pred))
print(confusion_matrix(y_test, lr_pred))

perceptron_model = Perceptron(max_iter=1000)
perceptron_model.fit(X_train, y_train)
perceptron_pred = perceptron_model.predict(X_test)
print("Perceptron Model: \n",classification_report(y_test, perceptron_pred))
print(confusion_matrix(y_test, perceptron_pred))

plt.figure(figsize=(20,15))
fpr, tpr, _ = roc_curve(y_test, lr_pred)
roc_auc = auc(fpr, tpr)
plt.subplot(2, 2, 1)
plt.plot(fpr, tpr, lw=2, label=f'(area = {roc_auc:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Logistic Regression')
plt.legend(loc='lower right')

fpr, tpr, _ = roc_curve(y_test, nb_pred)
roc_auc = auc(fpr, tpr)
plt.subplot(2, 2, 2)
plt.plot(fpr, tpr, lw=2, label=f'(area = {roc_auc:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Naive Bayes')
plt.legend(loc='lower right')

fpr, tpr, _ = roc_curve(y_test, perceptron_pred)
roc_auc = auc(fpr, tpr)
plt.subplot(2, 2, 3)
plt.plot(fpr, tpr, lw=2, label=f'(area = {roc_auc:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Perceptron')
plt.legend(loc='lower right')

fpr, tpr, _ = roc_curve(y_test, dt_pred)
roc_auc = auc(fpr, tpr)
plt.subplot(2, 2, 4)
plt.plot(fpr, tpr, lw=2, label=f'area = {roc_auc:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Decision Tree')
plt.legend(loc='lower right')

